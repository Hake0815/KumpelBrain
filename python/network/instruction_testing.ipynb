{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d511ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import instruction_test_data as itd\n",
    "import numpy as np\n",
    "from card_embedding import FilterEmbedding, SharedEmbeddingHolder, NormalizedLinear, AttackDataEmbedding, DiscardDataEmbedding, CardAmountDataEmbedding, ReturnToDeckTypeDataEmbedding, PlayerTargetDataEmbedding\n",
    "import nesting\n",
    "from multi_head_attention import MultiHeadAttention\n",
    "\n",
    "d=8\n",
    "shared_embedding_holder = SharedEmbeddingHolder(d, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934927f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_types, instruction_indices, instruction_data_types, instruction_data_type_indices, instruction_data, instruction_data_indices = nesting.flatten_instructions(itd.instructions_batch, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6589e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([ 0, 10], device='cuda:0'), tensor([ 0, 70], device='cuda:0')],\n",
       " [tensor([2], device='cuda:0'),\n",
       "  tensor([1], device='cuda:0'),\n",
       "  tensor([1], device='cuda:0')],\n",
       " [tensor([2, 2, 8], device='cuda:0'),\n",
       "  tensor([0, 1, 7], device='cuda:0'),\n",
       "  tensor([-1, -1, 10], device='cuda:0'),\n",
       "  tensor([1, 1, 5], device='cuda:0'),\n",
       "  tensor([-1, -1, 10], device='cuda:0'),\n",
       "  tensor([2, 2, 7], device='cuda:0'),\n",
       "  tensor([1, 1, 4], device='cuda:0'),\n",
       "  tensor([-1, -1, 10], device='cuda:0')],\n",
       " [tensor([0, 9], device='cuda:0')],\n",
       " [{'Operands': [],\n",
       "   'LogicalOperator': 0,\n",
       "   'IsLeaf': True,\n",
       "   'Condition': {'Field': 2, 'Operation': 0, 'Value': -1}},\n",
       "  {'Operands': [],\n",
       "   'LogicalOperator': 0,\n",
       "   'IsLeaf': True,\n",
       "   'Condition': {'Field': 3, 'Operation': 1, 'Value': 1}},\n",
       "  {'Operands': [{'Operands': [],\n",
       "     'LogicalOperator': 0,\n",
       "     'IsLeaf': True,\n",
       "     'Condition': {'Field': 3, 'Operation': 1, 'Value': 1}},\n",
       "    {'Operands': [],\n",
       "     'LogicalOperator': 0,\n",
       "     'IsLeaf': True,\n",
       "     'Condition': {'Field': 4, 'Operation': 1, 'Value': 8}}],\n",
       "   'LogicalOperator': 2,\n",
       "   'IsLeaf': False,\n",
       "   'Condition': None},\n",
       "  {'Operands': [],\n",
       "   'LogicalOperator': 0,\n",
       "   'IsLeaf': True,\n",
       "   'Condition': {'Field': 1, 'Operation': 0, 'Value': -1}}],\n",
       " [tensor([0], device='cuda:0')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64df1576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(2, 0, 0), (3, 0, 0)],\n",
       " [(0, 1, 0), (0, 6, 0), (1, 3, 0)],\n",
       " [(0, 0, 0),\n",
       "  (0, 2, 0),\n",
       "  (0, 4, 0),\n",
       "  (1, 0, 0),\n",
       "  (1, 2, 0),\n",
       "  (4, 0, 0),\n",
       "  (4, 1, 0),\n",
       "  (4, 2, 0)],\n",
       " [(4, 3, 0)],\n",
       " [(0, 0, 1), (0, 2, 1), (1, 0, 1), (4, 1, 1)],\n",
       " [(0, 5, 0)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f762814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 10],\n",
      "        [ 0, 70]], device='cuda:0')\n",
      "tensor([0, 0], device='cuda:0')\n",
      "tensor([10, 70], device='cuda:0')\n",
      "tensor([[ 0.8276, -2.0796, -0.4522,  1.1406,  0.7865,  0.3309,  0.1415, -0.3769],\n",
      "        [ 0.9722, -1.9412, -0.5720,  1.2607,  0.8419,  0.4174,  0.1827, -0.3138]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "attack_data= torch.stack(instruction_data[0])\n",
    "print(attack_data)\n",
    "print(attack_data[:,0])\n",
    "print(attack_data[:,1])\n",
    "attack_data_embedding = AttackDataEmbedding(8, device='cuda')\n",
    "print(attack_data_embedding(attack_data))\n",
    "print(attack_data_embedding(attack_data).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec2c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 1], device='cuda:0')\n",
      "tensor([[ 1.1419,  0.4338, -1.4672,  0.1070, -2.7960, -0.4082, -0.1120, -0.7619],\n",
      "        [-0.7828, -3.1895,  0.6799, -1.1064,  0.8218, -0.8566,  0.6500, -0.7707],\n",
      "        [-0.7828, -3.1895,  0.6799, -1.1064,  0.8218, -0.8566,  0.6500, -0.7707]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "discard_data= torch.tensor(instruction_data[1], device='cuda')\n",
    "print(discard_data)\n",
    "discard_data_embedding = DiscardDataEmbedding(8, device='cuda')\n",
    "print(discard_data_embedding(discard_data))\n",
    "print(discard_data_embedding(discard_data).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ed0641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2,  8],\n",
      "        [ 0,  1,  7],\n",
      "        [-1, -1, 10],\n",
      "        [ 1,  1,  5],\n",
      "        [-1, -1, 10],\n",
      "        [ 2,  2,  7],\n",
      "        [ 1,  1,  4],\n",
      "        [-1, -1, 10]], device='cuda:0')\n",
      "tensor([[ 2,  2],\n",
      "        [ 0,  1],\n",
      "        [-1, -1],\n",
      "        [ 1,  1],\n",
      "        [-1, -1],\n",
      "        [ 2,  2],\n",
      "        [ 1,  1],\n",
      "        [-1, -1]], device='cuda:0')\n",
      "tensor([ 8,  7, 10,  5, 10,  7,  4, 10], device='cuda:0')\n",
      "tensor([[ 1.2796,  0.2978, -1.6028,  1.4369, -1.7501, -0.1528,  1.6623, -2.3437],\n",
      "        [-0.8354,  1.1387,  0.1663, -0.3706, -0.6742,  0.5042,  0.4181, -0.0689],\n",
      "        [ 1.4766,  0.2806,  2.9991, -1.9369,  0.8828, -1.3831, -0.4896,  0.5260],\n",
      "        [-1.3097, -0.7277,  0.8864,  1.4407, -1.2723, -0.2631,  0.3757,  0.2811],\n",
      "        [ 1.4766,  0.2806,  2.9991, -1.9369,  0.8828, -1.3831, -0.4896,  0.5260],\n",
      "        [-0.8348,  1.1400,  0.1638, -0.3697, -0.6754,  0.4995,  0.4165, -0.0685],\n",
      "        [-0.5445,  0.3417,  0.8721,  0.6408,  0.8297, -0.0892,  0.0074,  0.0819],\n",
      "        [ 1.4766,  0.2806,  2.9991, -1.9369,  0.8828, -1.3831, -0.4896,  0.5260]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "card_amount_data= torch.stack(instruction_data[2])\n",
    "print(card_amount_data)\n",
    "print(card_amount_data[:,0:2])\n",
    "print(card_amount_data[:,2])\n",
    "card_amount_data_embedding = CardAmountDataEmbedding(shared_embedding_holder, d, device='cuda')\n",
    "print(card_amount_data_embedding(card_amount_data))\n",
    "print(card_amount_data_embedding(card_amount_data).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a82e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 9]], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([[-0.6631,  0.8867,  0.4409,  1.2479, -0.8346, -0.4733, -1.2001,  0.0907]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "return_to_deck_type_data = torch.stack(instruction_data[3])\n",
    "print(return_to_deck_type_data)\n",
    "print(return_to_deck_type_data[:, 0])\n",
    "print(return_to_deck_type_data[:, 1])\n",
    "return_to_deck_type_data_embedding = ReturnToDeckTypeDataEmbedding(\n",
    "    shared_embedding_holder, d, device='cuda'\n",
    ")\n",
    "embedded_return_to_deck_type_data = return_to_deck_type_data_embedding(return_to_deck_type_data)\n",
    "print(embedded_return_to_deck_type_data)\n",
    "print(embedded_return_to_deck_type_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07d9afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Operands': [], 'LogicalOperator': 0, 'IsLeaf': True, 'Condition': {'Field': 2, 'Operation': 0, 'Value': -1}}, {'Operands': [], 'LogicalOperator': 0, 'IsLeaf': True, 'Condition': {'Field': 3, 'Operation': 1, 'Value': 1}}, {'Operands': [{'Operands': [], 'LogicalOperator': 0, 'IsLeaf': True, 'Condition': {'Field': 3, 'Operation': 1, 'Value': 1}}, {'Operands': [], 'LogicalOperator': 0, 'IsLeaf': True, 'Condition': {'Field': 4, 'Operation': 1, 'Value': 8}}], 'LogicalOperator': 2, 'IsLeaf': False, 'Condition': None}, {'Operands': [], 'LogicalOperator': 0, 'IsLeaf': True, 'Condition': {'Field': 1, 'Operation': 0, 'Value': -1}}]\n",
      "tensor([[-0.5648, -1.2544, -0.6870, -1.0572, -0.7275,  1.1842, -1.5258, -0.6870],\n",
      "        [ 0.5402,  0.8311,  1.0681,  1.2784, -2.4457,  1.9160, -2.0477, -1.8444],\n",
      "        [ 3.1468,  0.9788, -4.7799, -0.0609, -1.0961,  3.9680, -3.7570, -3.6892],\n",
      "        [-0.7005,  0.6115,  0.5130,  0.7611,  0.7567,  0.3726,  0.0793, -0.5155]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "filter_data = instruction_data[4]\n",
    "print(filter_data)\n",
    "filter_embedding = FilterEmbedding(shared_embedding_holder, d, device='cuda')\n",
    "embedded_filter_data = filter_embedding.forward_v2(filter_data)\n",
    "print(embedded_filter_data)\n",
    "print(embedded_filter_data.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3293cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n",
      "tensor([[-2.4971, -0.3819,  0.3101,  0.2596,  0.0289,  1.5154, -1.0115,  0.8181]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "player_target_data = torch.tensor(instruction_data[5], device='cuda')\n",
    "print(player_target_data)\n",
    "player_target_data_embedding = PlayerTargetDataEmbedding(shared_embedding_holder, d, device='cuda')\n",
    "embedded_player_target_data = player_target_data_embedding(player_target_data)\n",
    "print(embedded_player_target_data)\n",
    "print(embedded_player_target_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "510f6eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 8])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [0, 3],\n",
      "        [0, 4],\n",
      "        [0, 5],\n",
      "        [0, 6],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [1, 3],\n",
      "        [2, 0],\n",
      "        [3, 0],\n",
      "        [4, 0],\n",
      "        [4, 1],\n",
      "        [4, 2],\n",
      "        [4, 3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "class InstructionEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shared_embedding_holder: SharedEmbeddingHolder,\n",
    "        dimension_out: int,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.dimension_out = dimension_out\n",
    "        self.attack_data_embedding = AttackDataEmbedding(\n",
    "            dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.discard_data_embedding = DiscardDataEmbedding(\n",
    "            dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.card_amount_data_embedding = CardAmountDataEmbedding(\n",
    "            shared_embedding_holder, dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.return_to_deck_type_data_embedding = ReturnToDeckTypeDataEmbedding(\n",
    "            shared_embedding_holder, dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.filter_embedding = FilterEmbedding(\n",
    "            shared_embedding_holder, dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.player_target_data_embedding = PlayerTargetDataEmbedding(\n",
    "            shared_embedding_holder, dimension_out, **self.factory_kwargs\n",
    "        )\n",
    "        self.instruction_type_embedding = nn.Embedding(\n",
    "            8, dimension_out, padding_idx=0, **self.factory_kwargs\n",
    "        )\n",
    "        self.instruction_data_type_embedding = nn.Embedding(\n",
    "            6, dimension_out, padding_idx=0, **self.factory_kwargs\n",
    "        )\n",
    "        self.data_multi_head_attention = MultiHeadAttention(\n",
    "            dimension_out, dimension_out, dimension_out, max(dimension_out//16, 4), 4, **self.factory_kwargs\n",
    "        )\n",
    "\n",
    "    def forward(self, instructions_batch: list[list[dict]]) -> torch.Tensor:\n",
    "        (\n",
    "            instruction_types,\n",
    "            instruction_indices,\n",
    "            instruction_data_types,\n",
    "            instruction_data_type_indices,\n",
    "            instruction_data,\n",
    "            instruction_data_indices,\n",
    "        ) = nesting.flatten_instructions(instructions_batch, **self.factory_kwargs)\n",
    "        instruction_type_embeddings = self.instruction_type_embedding(instruction_types)\n",
    "        instruction_data_type_embeddings = self.instruction_data_type_embedding(\n",
    "            instruction_data_types\n",
    "        )\n",
    "        attack_data_embeddings = self.attack_data_embedding(\n",
    "            torch.stack(instruction_data[0])\n",
    "        )\n",
    "        discard_data_embeddings = self.discard_data_embedding(\n",
    "            torch.tensor(instruction_data[1], **self.factory_kwargs)\n",
    "        )\n",
    "        card_amount_data_embeddings = self.card_amount_data_embedding(\n",
    "            torch.stack(instruction_data[2])\n",
    "        )\n",
    "        return_to_deck_type_data_embeddings = self.return_to_deck_type_data_embedding(\n",
    "            torch.stack(instruction_data[3])\n",
    "        )\n",
    "        filter_embeddings = self.filter_embedding(instruction_data[4])\n",
    "        player_target_data_embeddings = self.player_target_data_embedding(\n",
    "            torch.tensor(instruction_data[5], **self.factory_kwargs)\n",
    "        )\n",
    "\n",
    "        sorted_data = self.sort_tensors_with_respect_to_index(\n",
    "            (\n",
    "                attack_data_embeddings,\n",
    "                discard_data_embeddings,\n",
    "                card_amount_data_embeddings,\n",
    "                return_to_deck_type_data_embeddings,\n",
    "                filter_embeddings,\n",
    "                player_target_data_embeddings,\n",
    "            ),\n",
    "            instruction_data_indices,\n",
    "        )\n",
    "        data_tensors = sorted_data + instruction_data_type_embeddings\n",
    "\n",
    "        instruction_embeddings = self.embed_instruction_data(instruction_indices, instruction_data_type_indices, instruction_type_embeddings, data_tensors)\n",
    "\n",
    "        return (\n",
    "            instruction_embeddings,\n",
    "            instruction_indices,\n",
    "        )\n",
    "\n",
    "    def sort_tensors_with_respect_to_index(self, tensors, indices):\n",
    "        return torch.stack(\n",
    "            [\n",
    "                tensor\n",
    "                for _, tensor in sorted(\n",
    "                    zip(\n",
    "                        chain.from_iterable(indices),\n",
    "                        chain.from_iterable(tensors),\n",
    "                    ),\n",
    "                    key=lambda pair: pair[0],\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def embed_instruction_data(self, instruction_indices, instruction_data_type_indices, instruction_type_embeddings, data_tensors):\n",
    "        query_list = []\n",
    "        for i, instruction_index in enumerate(instruction_indices):\n",
    "            unbatched_query = torch.cat(\n",
    "                [\n",
    "                    instruction_type_embeddings[i].unsqueeze(0),\n",
    "                    data_tensors[\n",
    "                        (\n",
    "                            instruction_data_type_indices[:, 0:2] == instruction_index\n",
    "                        ).sum(1)\n",
    "                        == 2\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "            query_list.append(unbatched_query)\n",
    "        query_tensor = torch.nested.nested_tensor(query_list, layout=torch.jagged)\n",
    "        return (\n",
    "            query_tensor\n",
    "            + self.data_multi_head_attention(query_tensor, query_tensor, query_tensor)\n",
    "        ).sum(1)\n",
    "\n",
    "\n",
    "instruction_embedding = InstructionEmbedding(shared_embedding_holder, d, device=\"cuda\")\n",
    "(\n",
    "    instruction_embeddings,\n",
    "            instruction_indices,\n",
    ") = instruction_embedding(itd.instructions_batch)\n",
    "print(instruction_embeddings.size())\n",
    "print(instruction_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
